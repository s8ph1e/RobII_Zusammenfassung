Fundamentale Fragen:
\begin{itemize}
	\item Was ist Entscheiden?
	\item Welche Arten von Systemen fällen Entscheidungen?
	\item Welche ist der allgemeinste Rahmen für eine Definition von Entscheidungssystemen?
\end{itemize}

\subsection{Der rationale Agent}
\begin{itemize}
	\item Allgemeine Definition eines in eine Umwelt eingebetteten, handelnden Systems
	\item Beliebige Art von Umwelt
	\item Definiert durch den Agentenzyklus: \textbf{Perzeption -- Entscheidung -- Handlung} (\autoref{ch07_agentenzyklus})
	\item Der Agentenzyklus ist eine andere Darstellung eines Regelkreises
	\item Entscheidung ist eine abstrakte Form der Regelung
\end{itemize}

\begin{figure}[ht]\centering 
\includegraphics[width=0.8\textwidth]{figures/07_agentenzyklus.png}
\caption{Der Agentenzyklus: Jede Modellbildung der Welt ist starke Reduktion}
\label{ch07_agentenzyklus}
\end{figure}

\subsubsection{Beispiele}
\paragraph{Schachcomputer}
\begin{itemize}
	\item Keine physische Umwelt
	\item Sehr beschränkte Umwelt
	\item Nur ein Ziel: das Spiel zu gewinnen
	\item Sensorik: Das Spielfeld wird komplette wahrgenommen
	\item Aktorik: Durchführung eines symbolischen Zugs
	\item Umwelt: semi-statisch, episodisch, diskret, deterministisch, vollständig beobachtbar, Multiagent
\end{itemize}
\paragraph{Industrieroboter}
\begin{itemize}
	\item Oftmals, bei Abwesenheit von Sensorik, kein Agent, sondre nur ein Werkzeug
	\item Physische Umwelt
	\item Beschränkte Umwelt
	\item Wenn überhaupt rationaler Agent, üblicherweise extrem begrenzter Entscheidungsrahmen
	\item Ziel: Erfolgreiche Ausführung einer Operation
	\item Umwelt: dynamisch, episodisch, diskret oder kontinuierlich, deterministisch, vollständig beobachtbar, Einzelagent oder Multiagent
\end{itemize}
\paragraph{Mensch}
\begin{itemize}
	\item Extrem komplexe Umwelt
	\item Sehr mächtige Perzeption
	\item Sehr wissensbasiert
	\item Vielschichtige Motivationen und Zielsetzungen
	\item Umwelt: dynamisch, sequentiell, kontinuierlich, stochastisch, unvollständig beobachtbar, Multiagent
\end{itemize}
\paragraph{Serviceroboter}
\begin{itemize}
	\item Anforderungen dem Menschen viel ähnlicher als einem Industrieroboter
	\item Sehr komplexe Umwelt
	\item Vielschichtige Zielsetzung
	\item Es ist bei Servicerobotern ein ganz anderes Vorgehen, als bei Industrierobotern erforderlich
	\item Umwelt: wie beim Menschen
\end{itemize}

\subsubsection{Umwelt}
Für die verschiedenen Agenten werden sehr unterschiedliche Umgebungen benötigt.
Ein Rahmen für eine nützliche und eindeutige Charakterisierung von Umgebungen ist nötig.
\paragraph{Anforderungen}
\begin{itemize}
	\item Die Charakterisierung muss fundamentale, nicht oberflächliche Eigenschaften aufzeigen
	\item Es muss ein Rahmen gebildet werden, in den verschiedenen Verfahren und Paradigmen eingeordnet werden können
	\item eine Leitlinie für die Entwicklung neuer Verfahren muss gebildet werden
\end{itemize}
Umwelten verschiedener Agenten können gut durch einige fundamentale, duale Eigenschaften unterschieden werden.
\paragraph{Eigenschaften} \mbox{}
\vspace{1em} \\
\begin{tabular}{p{0.4\textwidth} p{0.1\textwidth} p{0.4\textwidth}}
\textbf{statisch} & \centering vs.\ & \textbf{dynamisch}\\
Der Agent ist das einzige Element, welches den Zustand der Umwelt verändert
& &
Die Umwelt kann sich auch ohne zutun des Agenten verändern
\\
\textbf{episodisch} & \centering vs.\ & \textbf{sequentiell}\\
Der zeitliche Verlauf des Geschehens  ist in abgeschlossene Einheiten unterteilt, zwischen denen keinerlei kausal Zusammenhänge bestehen
& &
Die vollständige zeitliche Vergangenheit hat Auswirkungen auf die Gegenwart ,die Gegenwart auf die gesamte Zukunft
\\
\textbf{diskret} & \centering vs.\ & \textbf{kontinuierlich}\\
Die Zustände der Umwelt sind diskret, ebenso der zeitliche Verlauf des Geschehens
& &
Der Zustandsraum der Umwelt ist kontinuierlich und die Zeit fließt ebenfalls kontinuierlich
\\
\textbf{deterministisch} & \centering vs.\ & \textbf{stochastisch}\\
Der Ausgang einer jeden Handlung ist eindeutig bestimmt
& &
Eine Handlung kann mit bestimmten Wahrscheinlichkeiten zu verschiedenen Ausgängen führen
\\
\textbf{vollständig beobachtbar} & \centering vs.\ & \textbf{unvollständig beobachtbar}\\
Der Agent kann die komplette Umwelt jederzeit vollständig und exakt wahrnehmen
& &
Der Agent kann die Umwelt nur eingeschränkt und fehlerbehaftet wahrnehmen
\\
\textbf{Einzelagent} & \centering vs.\ & \textbf{Multiagent}\\
Nur eine als Agent modellierbare Entität agiert in der Umwelt
& &
Viele als Agenten modellierbare Entitäten agieren in der Umwelt und können kooperieren oder konkurrieren
\end{tabular}

Die Beschreibung der Umwelt durch das jeweils komplexere Merkmal wird nur gewählt, wenn das einfachere Merkmal im Szenario des Agenten keine hinreichende Beschreibung ist.\\
\textit{Hinreichend} ist hierbei auch in dem Sinne zu verstehen, dass es oftmals noch keine ausreichenden Verfahren gibt, um die komplexeren Merkmale zu berücksichtigen.

\subsubsection{Utility -- Motivation des Agenten}
\begin{itemize}
	\item Ein Agent benötigt Motivation um Absichten als Fundament für Entscheidungen
	\item In der Entscheidungstheorie durch das Konzept der \textbf{Utility} abgebildet (Ursprung des Begriffs in der Ökonomie)
	\item Dieses Konzept ist deutlich allgemeiner, als spezielle Ziele z.B.\ in der logikbasierten Planung	
\end{itemize}

\paragraph{Konzept der Utility}
\begin{itemize}
	\item Die utility (auch: \textit{utility function}) modelliert in der Ökonomie ein numerisches Maß der Befriedigung eines Agenten (Menschen) durch einen bestimmten Konsum
	\item In der Robotik/KI wird jedoch ein Motivationsmaß eines Agenten nicht \emph{modelliert}, sondern \emph{konstruiert}
	\item Die uitility wird hier genutzt, um einem künstlichen Agenten bestimmte Motivationen einzupflanzen
	\item Die utility ist ein allgemeines Konzept: es können konkurrierende und ergänzende Zielsetzungen fusioniert werden
	\item Im Rahmen der utility können auch Absichten gegeneinander abgewogen werden
\end{itemize}

\subsubsection{Problem}
Die urspr\"ungliche, klassische Aktionsplanung nimmt an, dass die Umwelt statisch, episodisch, diskret, deterministisch und vollst\"andig beobachtbar ist und zudem konkreten, nicht \"uberlappenden Ziele verfolgt. 
F\"ur eine dynamische, sequentielle und kontinuierliche Umwelt gibt es Erweiterungen des klassischen Aktionsplanens, im Hinblick \textbf{stochastisch}, \textbf{vollst\"andig beobachtbar} und \textbf{unterschiedlich stark konkurrierende Ziele} jedoch nicht.
%
\begin{figure}[ht]
	\centering 
	\begin{subfigure}{.45\textwidth}
		\includegraphics[width=\textwidth]{figures/ch07_stochastischeUmwelt.png}
		\caption{Stochastische Umwelt}
		\label{ch07_stumwelt}
	\end{subfigure}
	\begin{subfigure}{.4\textwidth}
		\includegraphics[width=\textwidth]{figures/ch07_umwelt-bsp.png}
		\caption{Beispiel}
		\label{ch07_umwelt-bsp}
	\end{subfigure}
\end{figure}

\autoref{ch07_stumwelt} zeigt eine stochastische Umwelt.
\"Ubergangswahscheinlichtkeiten beschreiben hierbei den Ausgang von Handlungen.
In stochastischen Umgebungen ist daher es essenziell, dass mit numerischen Wahrscheinlichkeiten umgegangen werden kann, jedoch ist das in der klassischen, logikbasierten Aktionsplanung nicht der Fall, weshalb \textbf{probabilistische Methoden} (wie MDPs) zum Einsatz kommen.
%Exkurs zu Bayes F29+30
\subsection{Markov Prozesse}
\begin{itemize}
\item ein verbreitetes Werkzeug f\"ur die Modellierung stochastischer Prozesse
\item zentrale Eigenschaft: die zuk\"unftige Entwicklung kann durch Kenntnis einer \textbf{begrenzten} Vorgeschichte prognostiziert werden
\item erst diese Markoveigenschaft erlaubt die Prognose bez\"uglich der Zukunft in einem solchen stochastischen Prozess
\item es gibt zwei Arten von Markov Prozessen: zeitdiskrete und zeitstetige Markov Prozesse
\end{itemize}

\paragraph{Zeitdiskreter Markov Prozess:}
\begin{itemize}
\item werden auch Ketten genannt
\item \"Uberg\"ange in einer Markov Kette sind Spr\"unge in einer diskreten Zeit
\item Mathematisch wird die Wahrscheinlichkeit f\"ur einen zuk\"unftigen Zustand als bedingte Wahrscheinlichkeit, abh\"angig von einer endlichen Zahl an vergangenen Zust\"anden modelliert
\item die Ordnung einer Markov Kette gibt an, welcher Ausschnitt der Vergangenheit eine Rolle spielt
\item die Ordnung ist immer endlich, sonst w\"are die Markov Eigenschaft nicht erf\"ullt
\item je geringer die Ordnung, desto einfach die Modellierung
\end{itemize}

\textbf{Markovkette 1. Ordnung}:
\begin{itemize}
\item nur die Gegenwart spielt eine Rolle f\"ur die Betrachtung der Zukunft
\item die Wahrscheinlichkeit des Nachfolgezustands h\"angt nur vom aktuellen Zustand ab
\item $Pr(X_{n+1} = x | X_n = x_n, \ldots, X_1 = x_1, X_0 = x_0) = Pr(X_{n+1}|X_n = x_n)$
\end{itemize}

Markovketten: Modellierung der Welt
\begin{itemize}
	\item Markovketten k\"onnen von Agenten zur Modellierung von stochastischen Umgebungen genutzt werden
	\item bei diskreten Markovketten muss die Welt dazu in eine diskrete Zustandsmenge zerlegt werden
	\item die Zerlegung kann f\"ur verschiedene Umweltaspekte getrennt erfolgen, z.B. Roboterposition, Objektform oder Intention eines Menschen
	\item Welt als diskrete Zust\"ande: einfache Beispiel: Unterteilung von R\"aumen in diskrete Regionen (die Position eines mobilen Roboters wird durch die Region, in der er sich befindet, repr\"asentiert)
	\item Welt als stochastischer Prozess: Bestimmte \textbf{Aufl\"oser} f\"uhren zu stochastischen Zustands\"uberg\"angen (Bayesianische Modellierung), z.B. Handlung eines Agenten:\\ Beispiel:
	\begin{itemize}
		\item Der Roboter steht vor einem Mensch, es fand noch keine Kommunikation statt
		\item M\"oglichkeit 1: Roboter sagt: Folge mir!; damit ergibt sich eine Wahrscheinlichkeit von $75\%$, dass der Mensch interessiert ist und die Intention hat zu folgen und $25\%$, dass der Mensch keine Lust hat, also nicht die Intention entwickelt zu folgen
		\item M\"oglichkeit 2: Roboter sagt: Ich habe etwas interessantes zu zeigen. Folge mir bitte!; damit steigt die Wahrscheinlichkeit, dass der Mensch folgen will auf $90\%$, wohingegen er mit $10\%$iger Wahrscheinlichkeit nicht folgen will
		\item d.h. durch die Wahl der Handlung kann der Agent die \textbf{\"Ubergangswahrscheinlichkeiten steuern}!!!!!
	\end{itemize}
\end{itemize}

\subsubsection{Markov Entscheidungsprozesse}
Ketten von Handlungen bzw. Ausl\"osern f\"uhren zu Bayes-Netzwerken. F\"ur die Entscheidungsfinder in diesen stochastischen Prozessen existiert ein fundiertes Modellierungsrahmenwerk spezieller Bayes-Netze: \textbf{Markov Decision Processes} (MDPs)

\paragraph{Struktur}
\begin{itemize}
\item Eine Menge von symbolischen Zust\"anden der Welt S (d.h. die Umwelt bei einem MDP ist diskret)
\item Eine Menge von symbolischen Handlung des Agenten U
\item Ein \"Ubergangsmodell (transition model) $T(s', u, s)$ welches die stochastischen \"Uberg\"ange modelliert (modelliert den Markov-Prozess)
\item Ein Belohnungsmodell (reward model) $R(s,u)$ welches die Ziele und Absichten des Agenten modelliert
\item Startzustand $s_0$
\end{itemize}
Zust\"ande und Aktionen m\"ussen dabei einzigartig sein.
Es gilt au{\ss}erdem die Markov-Eigenschaft, d.h. alle Wahrscheinlichkeiten h\"angen immer nur vom aktuellen Zustand ab und nicht von den bereits vergangenen.
Die in jedem Schritt aufaddierte Belohnungen werden \emph{Utilitiy} genannt und die Aufgabe des Agenten besteht darin, Aktionen mit hoher Belohnung zu w\"ahlen, um die Utility zu maximieren.

\paragraph{Statische Eigenschaften}
\begin{itemize}
	\item In einem MDP ist die Welt zu jedem Zeitpunkt in \textbf{genau einem} Zustand
	\item In einem diskreten MDP ist dies ein symbolischer Zustand, welcher alle Eigenschaften der Welt repr\"asentiert
	\item Der Agent kann in jedem Agentezyklus \textbf{genau eine} Handlung ausf\"uhren
	\item In einem diskreten MDP ist dies eine symbolische Handlung
	\item Im reinen MDP nimmt der Agent den aktuellen Zustand \textbf{immer und eindeutig} wahr
\end{itemize}

\paragraph{Transitionsmodell}
\begin{itemize}
	\item $T(s', u, s) = p(\text{transition})$: wenn der alte Zustand $s$ war und der Agent die Handlung $u$ ausgef\"uhrt hat, dann beschreibt dieser Eintrag im Modell die Wahrscheinlichkeit, dass der neue Zustand $s'$ sein wird
	\item Das Transitionsmodell ist ein Tensor 3. Stufe
	\item In der Praxis oft als Vektor (\"uber U) von Matrixen $(S, S')$ dargestellt: $|T| = |S| * |S| * |U|$
	\item Die Gr\"o{\ss}e w\"achst also kubisch
\end{itemize}

\paragraph{Belohnungsmodell}
\begin{itemize}
	\item $R(s,u) = \text{Belohnung oder Strafe}$: wenn der Zustand $s$ war und der Agent f\"uhrt die Handlung $u$ aus, erh\"alt er die sofortige Belohnung (oder negativer Belohnung, also Strafe) wie vom Modell an $R(s,u)$ gegeben
	\item Das Belohnungsmodell ist eine Matrix
	\item Die Eintr\"age k\"onnen beliebige Realzahlen sein, je nach Modellierung
	\item Durch das Belohnungsmodell k\"onnen auch konkurrierende oder erg\"anzende Zielsetzungen modelliert werden
\end{itemize}

\paragraph{Ziel des Agenten}
\begin{itemize}
	\item Das Ziel des rationalen Agenten im MDP ist, die Summe der Belohnungen bis zu einem bestimmten Zeithorizont (im reinen MDP meistens unendlich) zu maximieren
	\item Dies f\"uhrt zu einer \textbf{Gewinnmaximierung} bzw. \textbf{Risikoabsch\"atzung} in die Zukunft als Grundlage f\"ur den Entscheidungsprozess
	\item Das einzige explizite Ziel des Agenten ist die Gewinnmaximierung
	\item Sie bezieht implizit alle gegebenen Zielsetzungen in den Entscheidungsprozess mit ein
\end{itemize}

\paragraph{Gewinnmaximierung}
\begin{itemize}
	\item Da in einer stochastischen Welt keine festen Handlungsketten existieren, wird eine Entscheidungsfunktion (\emph{policy}) berechnet
	\item Die Entscheidungsfunktion $\pi$ gibt f\"ur jeden Zustand $s$ eine Handlung zur\"uck: $\pi(s)$
	\item Ein Agent in einem MDP muss nur eine optimale Entscheidungsfunktion $\pi^*$ berechnen, welche immer die Handlung mit der gr\"o{\ss}ten, langfristigen Gewinnerwartung w\"ahlt: $\pi^* (s)$
	\item Es gibt immer mindestens eine $\pi^* (s)$, welche zu einem gegebenen MDP Modell optimale Entscheidungen liefert
	\item Bei unendlichem Horizont ist die optimale Entscheidungsfunktion station\"ar: es gibt f\"ur einen Zustand immer dieselbe optimale Handlung
	\item In einem voll beobachtbaren (reinen) MDP ist die Variante mit unendlichem Horizont somit die einfachste
	\item Im Folgenden wird bei reinem MDPs nur der Fall des unendlichen Zeithorizonts ber\"ucksichtigt
\end{itemize}

\paragraph{Utility}
\begin{itemize}
	\item Die Utility einer Zustandskette ist die Summe ihrer Belohnungen bzw. Strafen:\\ $U_h(\left[s_0, s_1, s_2, \ldots \right]) = R(s_0) + R(s_1) + R(s_2) + \ldots$
	\item Die Einf\"uhrung eines Abschlags (\emph{discount}) $\gamma$ auf Zust\"ande  in der Zukunft erm\"oglicht eine endliche utility f\"ur alle unendlichen Zustandsketten:\\ $U_h(\left[s_0, s_1, s_2, \ldots \right]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \ldots$
\end{itemize}

\paragraph{Entscheidungsfunktion}
\begin{itemize}
	\item Es existieren verschieden Verfahren, um eine optimale Entscheidungsfunktion aus dem gegebenen MDP Modell $(S, A, T, B)$ zu berechnen.
	Unter anderem: Policy Iteration oder Reinforcement learning einer policy
	\item Das wichtigste aber ist: \textbf{Value Iteration} (zur Berechnung der optimalen policy)
\end{itemize}

\paragraph{Umgebungseigenschaften}
\begin{itemize}
	\item Dynamisch
	\item Sequenziell
	\item Diskret (aber auch kontinuierlich)
	\item Stochastisch
	\item \textbf{Vollst\"andig} beobachtbar
	\item Multiagent
\end{itemize}

\subsubsection{MDP Value Iteration}

\paragraph{Problem}
\begin{itemize}
	\item F\"ur jeden Zustand wird eine utility berechnet, die die erwartete utility aller m\"oglichen, folgenden Zustandssequenzen widerspiegelt
	\item Daraus kann die Entscheidungsfunktion direkt gewonnen werden
	\item Die Zustandssequenzen wiederum h\"angen von der Entscheidungsfunktion ab
	\item $\Rightarrow$ verschr\"anktes Problem
\end{itemize}

\paragraph{Ansatz}
\begin{itemize}
	\item iterativer Ansatz
	\item die wirkliche utility eines Zustands w\"are die utility bezogen auf die optimale policy $U_{real}(s) = U^{\pi^*}(s)$
	\item die wirkliche utility ist jedoch zu Beginn nicht bekannt, da die optimale policy noch berechnet werden muss
	\item zuerst wird eine beliebige policy $\pi_{start}$ gew\"ahlt
	\item dazu wird jedem Zustand eine initiale utility zugewiesen
	\item die tats\"achliche utility jedes Zustands, bezogen auf eine policy $\pi$, ist die Summe aller zuk\"unftigen Belohnungen:
	$U^{\pi}(s) = E \left[ \sum_{t = 0}^{\infty} \gamma^t R(s_t, u_t) | \pi, s_0 = s \right]$
	\item die Summe der zuk\"unftigen Belohnungen h\"angt jedoch von der Handlungswahl und damit von $\pi$ ab
\end{itemize}

\paragraph{Handlungswahl}
\begin{itemize}
	\item es kann nun eine policy gefunden werden, die auf den aktuell zugewiesenen utilites basiert und I-Schritt weit optimal ist
	\item nach der utility FUnktion wird nun die Handlung, die die erwartete zuk\"unftige Belohnung maximiert gew\"ahlt: $\pi^{1step*}(s) = argmax_u \sum_{s'} T(s', u, s) U(s')$
	\item durch diese Formel wird auch ein Zusammenhang zwischen einem Zustand und seinem Nachbarn gebildet
\end{itemize}

\paragraph{Iterationsschritt}
\begin{itemize}
	\item nun wird die Zuweisung wieder in die utility-Berechnung zur\"uckgef\"uhrt
	\item es entsteht eine Iterationsformel zwischen benachbarten Zust\"anden:
	$U(s) = \gamma max_u (R(s,u) + \sum_{s'} T(s', u, s) U(s')) = \text{Bellmann Formel}$
	\item somit kann r\"uckw\"arts von $\text{utility}(s)$ auf $\text{utility}(s')$ geschlossen werden
\end{itemize}

\paragraph{Iterationsablauf}
\begin{itemize}
	\item Beginn: irgendwelche niedrigen Werte f\"ur die utilites aller Zust\"ande
	\item Iterationsschritt: Anwendung der Bellmann Formel auf jeden Zustand unter Nutzung aller seiner Nachbarn
	\item Vielfach Anwendung des Iterationsschritts auf alle Zust\"ande
	\item Irgendwann wird ein Gleichgewicht erreicht, das die optimale policy $\pi^*$ ist
	\item Die Iteration konvergiert zur optimalen policy
	\item In der Praxis werden die Iterationen bis zu einer akzeptablen Konvergenzschranke durchgef\"uhrt
\end{itemize}

\paragraph{Eigenschaften}
\begin{itemize}
	\item MDP Value Iteration ist eine Kontraktion: der Algorithmus konvergiert immer
	\item In der Praxis konvergiert er bei kleinerem $\gamma$ schneller
	\item MDP Value Iteration ist verh\"altnism\"a{\ss}ig schnell (im Vergleich zu anderen Verfahren und anderen Arten von MDPs)
	\item Fast optimale Entscheidungsfunktionen sind in der Praxis ausreichend
\end{itemize}

\subsubsection{POMDP (Partially Observable Markov Decision Processes)}
Eine Erweiterung von MDPs f\"ur unvollst\"andig beobachtbare Umgebungen (Perzeption \"ublicherweise verrauscht und deckt nur einen Teil des Szenarios ab)

\paragraph{Zustandssch\"atzung}
\begin{itemize}
	\item Die Sch\"atzung des aktuellen Zustands muss nicht nur auf der Perzeption basieren
	\item Grundprinzip: rationaler Agent in Dom\"ane mit Markov Eigenschaft:
	\begin{itemize}
		\item Aktueller Zustand kann \"uber unvollkommene Perzeption \textbf{beobachtet} werden (measurement)
		\item Aktueller Zustand kann auf Basis des vorherigen Zustands und der ausgef\"uhrten Handlung \textbf{vorhergesagt} werden (prediction)
	\end{itemize}
	\item Die \textbf{Beobachtung} wird durch ein auf die Eigenschaften des physischen oder logischen Sensors abgestimmtes Unsicherheitsmodell abgebildet
	\item Die Unsicherheit kann so explizit, numerisch angegebenw erden
	\item Diese Unsicherheit kann verschieden modelliert werde:
	\begin{itemize}
		\item kontinuierlich oder diskret
		\item parametrisch oder nicht-parametrisch
	\end{itemize}
	\item Die \textbf{Vorhersage} wird \"uber ein Transitionsmodell realisiert
	\item Das Transitionsmodell f\"uhrt zu einer Wahrscheinlichkeitsverteilung \"uber allen Zust\"anden wie im MDP
	\item Eine Vorhersage funktioniert nur iterativ, ausgehend von einem aktuellen Zustand (bzw. begrenzten Historie: Markov Eigenschaft)
\end{itemize}

\paragraph{Bayes Filter} {\ }\\
Vorhersage und Beobachtung k\"onnen nun fusioniert werden, um zusammen eine h\"ohere Genauigkeit zu erreichen!\\

Idee
\begin{itemize}
	\item Eine gute Vorhersage kann eine sehr ungenaue Beobachtung verbessern
	\item Eine gute Beobachtung pr\"azisiert jedoch eine ungenaue Vorhersage
	\item Da die Vorhersage nur iterativ funktioniert, arbeitet der Bayes Filter ebenfalls iterativ von Zeit t-1 nach t 
\end{itemize}

Zustandsvermutung
\begin{itemize}
	\item Die Ausgabe einer Vorhersage und auch einer Beobachtung ist eine Wahrscheinlichkeitsverteilung
	\item Da der Bayes Filter iterativ l\"auft, ist die Eingabe also auch eine Wahrscheinlichkeitsverteilung
	\item Der Agent besitzt daher zu jedem Zeitpunkt eine \textbf{Zustandsvermutung} (belief)
	\item Die Zustandsvermutung repr\"asentiert die subjektive Wahrnehmung des Agenten bez\"uglich der nur unvollst\"andig beobachtbaren Welt (Zustandsraum)
	\item Die Zustandsvermutung ist eine
	\begin{itemize}
		\item diskrete Wahrscheinlichkeitsverteilung \"uber einem diskreten Zustandsraum
		\item oder eine kontinuierliche Verteilung \"uber einem kontinuierlichen Zustandsraum
	\end{itemize}
\end{itemize}

Vorhersage
\begin{itemize}
	\item Zust\"ande $x$, Aktionen $u$
	\item Diskreter Fall: $\overline{p}_{k,t} = \sum_i p(X_t = x_k | u_t, X_{t-1} = x_i)p_{i, t-1}$
	\item Kontinuierlicher Fall: $\overline{bel}(x_t) = \int p(x_t | u_t, x_{t-1}) bel(x_{t-1})dx_{t-1}$
\end{itemize}

Beobachtung
\begin{itemize}
	\item Zust\"ande $x$, Beobachtung $z$
	\item Diskreter Fall: $p_{k,t} = \eta p(z_t | X_t = x_k) \overline{p}(k,t)$
	\item Kontinuierlicher Fall: $bel(x_t) = \eta p(z_t | x_t) \overline{bel}(x_t)$
\end{itemize}

Diskreter Bayes Filter:\\
\autoref{fig:ch08:diskBayes}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/ch08_diskreterBayes.png}
  \caption{Volst\"andige diskrete Bayes Filter mit Zust\"anden $x$, Aktion $u$ und Beobachtung $z$}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/ch08_diskreterBayesAlter.png}
  \caption{Alternative Darstellung mit Normalisierer $\alpha$, Zust\"anden $s$, explizitem Transitionsmodell $T$ und Beobachtungsmodell $O$}
  \label{fig:sub2}
\end{subfigure}
\caption{Diskreter Bayes Filter}
\label{fig:ch08:diskBayes}
\end{figure}

Kontinuierlicher Bayes Filter:\\
\autoref{fig:ch08:kontBayes}
\begin{figure}
	\centering
  \includegraphics[width=0.5\linewidth]{figures/ch08_kontinuierlicherBayes.png}
\caption{Allgemeine kontinuierlicher Bayes Filter}
\label{fig:ch08:kontBayes}
\end{figure}
Kontinuierliche Bayes Filter werden durch spezielle Methoden realisiert.
Parametrische Filter:
\begin{itemize}
	\item Kalman Filter
	\item Extended Kalman Filter
	\item Unscented Kalman Filter
	\item Information Filter
	\item Extended Information Filter
\end{itemize}
Nicht-parametrische Filter: Partikelfilter\\

Um MDPs in einer unvollst\"andig beobachten Umgebung nutzen zu k\"onnen werden Bayes Filter mit MDPs verbunden.
Das erfordert die Einf\"uhrung der Zustandsvermutung in den MDP.
Dadurch hat der MDP zwei Komponenten:
\begin{itemize}
	\item Realer Zustand der Welt
	\item Subjektive Zustandsvermutung des Agenten
\end{itemize}
$\Rightarrow$ \textbf{POMDPs}

\paragraph{POMDP} {\ }\\
Umwelt:
\begin{itemize}
	\item Dynamisch
	\item Sequentiell
	\item Diskret oder Kontinuierlich
	\item Stochastisch
	\item Unvollst\"andig beobachtbar
\end{itemize}

Struktur/Modell:
\begin{itemize}
	\item Eine Menge von symbolischen Zust\"anden der Welt S
	\item Eine Menge von symbolischen Handlungen des Agenten U
	\item Eine Menge von symbolischen Beobachtungen des Agenten M
	\item Ein \"Ubergangsmodell (transition model) $T(s', u, s)$ welches die stochastischen \"Uberg\"ange modelliert
	\item Ein Belohnungsmodell (reward model) $R(s,u)$ welches die Ziele und Absichten des Agenten modelliert
	\item Ein Beobachtungsmodell (observation model) $O(m, s')$ welches die Ungenauigkeit der Wahrnehmung des Agenten modelliert
	\item Einen Startzustand $s_0$
\end{itemize}

Statische Eigenschaften
\begin{itemize}
	\item statischen Eigenschaften des reinen MDP
	\item + der Agent kann in jedem Agentenzyklus genau eine Beobachtung machen
	\item + in einem diskreten POMDP ist dies eine symbolische Beobachtung
\end{itemize}

Dynamikmodelle
\begin{itemize}
	\item Transitions- und Belohnungsmodell funktionieren wie beim MDP
	\item Beobachtungsmodell
	\begin{itemize}
		\item $O(m,s') =$ Wahrscheinlichkeit: wenn die Beobachtung $m$ gemacht wurde, dann ist die Wahrscheinlichkeit, dass der wirkliche Weltzustand $s'$ ist, vom Modell an $O(m,s'$ gegeben
		\item das Beobachtungsmodell ist eine Matrix
	\end{itemize}
\end{itemize}

Sekund\"are Eigenschaften
\begin{itemize}
	\item Die policy existiert auch beim POMDP, wenn auch in anderer Form
	\item Hinzu kommt die Zustandsvermutung
	\begin{itemize}
		\item Im diskreten Fall eine diskrete Wahrscheinlichkeitsverteilung \"uber alle Zust\"ande
		\item Die Zustandsvermutung wird zu jedem Zeitschritt durch Bayes Filterung neu berechnet
		\item F\"uhrt zu einer klaren Trennung zwischen objektiver Realit\"at (echter Weltzustand) und der subjektiven Sicht des Agenten
	\end{itemize}
\end{itemize}

Entscheidungsfunktion:
\begin{itemize}
	\item Bei MDPs enth\"alt die Entscheidungsfunktion eine Handlung zu jedem Zustand
	\item Im Gegensatz zu MDPs muss die Entscheidungsfunktion bei POMDPs \"uber \textbf{alle m\"oglichen Zustandsvermutungen} definiert sein
	\item Struktur der Entscheidungsfunktion
	\begin{itemize}
		\item Bei diskreten POMDPs: \"uber dem $|S|-1$ dimensionalen Raum aller m\"oglichen Zustandsvermutungen definiert
		\item Bei kontinuierlichen POMDPs: \"uber einem unendlich-dimensionalen Raum definiert, daher speziell \"uber endlich-dimensionalen Teilr\"aumen angegeben
	\end{itemize}
	\item Berechnung der Entscheidungsfunktion: aufgrund der komplexen Struktur ist die Berechnung deutlich komplizierter als bei MDPs (Value Iteration existiert, ist aber anders)
\end{itemize}

Raum der Vermutungen (diskreter POMDP)
\begin{itemize}
	\item Alle m\"oglichen Wahrscheinlichkeiten \"uber allen Zust\"anden $s$ eines Szenarios spannen den Raum der Zustandsvermutungen auf
	\item Da $\sum p(s_i) = 1$, ist der Raum begrenzt, also ein Simplex (= konvexe H\"ulle von $n+1$ Punkten in einem n-dimensionalen Raum)
	\item Wegen der Summe $|S|-1$ Dimensionen
\end{itemize}

\subsubsection{POMDP Value Iteration}
\begin{itemize}
	\item iterativer Ansatz (wie bei MDP)
	\item ein \textbf{endlicher Horizont} wird vorausgesetzt
	\item ein schrittweises Vorausschauen wird von einer initialen Entscheidungsfunktion aus durchgef\"uhrt
	\item das Vorausschauen muss die stochastische Natur der Weltdynamik, als auch die verschieden genauen Beobachtungen eben dieser Dynamik in die Abw\"agung einflie{\ss}en lassen
\end{itemize}

\paragraph{Struktur}
\begin{itemize}
	\item Lineare Funktionen $\alpha$ repr\"asentieren die utility kontinuierlich \"uber dem gesamten Zustandsvermutungs-Simplex bezogen auf eine Handlung (nicht Zustand, wie im MDP)
	\item f\"ur jede Zustandsvermutung kann aus den Funktionen eine utility berechnet werden
\end{itemize}

\paragraph{Beginn}
\begin{itemize}
	\item Der Algorithmus beginnt mit dem Pseudohorizont $0$
	\item Nun wird bis zu einem Horizont $T$ in die Zukunft iteriert
	\item Dabei wird eine Menge $\Gamma$ von linearen Funktionen $\alpha$ erzeugt, die Gewinnerwartungswerte f\"ur Mengen m\"oglicher Handlungsketten darstellen
\end{itemize}

\paragraph{Schritt} besteht aus zwei wesentlichen Teilen
\begin{enumerate}
	\item Tempor\"are Koeffizienten werden durch die Bayes Vorw\"artsfilterung (Projektion) jeder vorigen linearen Funktion \"uber jede m\"ogliche Handlung und Wahrnehmung erzeugt
	\begin{itemize}
		\item die Projektion jeder linearen Funktion $\alpha$ aus $\Gamma$ in $t-1$ durch jede Kombination von Aktion $u$ und Beobachtung $m$ f\"uhrt zu den tempor\"aren Koeffizienten $v\{ k \}$ (Code F. 42)
	\end{itemize}
	\item die tempor\"aren Koeffizienten werden f\"ur die Bildung der Erwartung \"uber alle m\"oglichen Beobachtungskombinationen verwendet, woraus die neuen linearen Funktionen erzeugt werden
	\begin{itemize}
		\item aus den tempor\"aren Koeffizienten wird f\"ur jede m\"ogliche Kombination von Beobachtungen die Gewinnerwartung berechnet und eine neue lineare Funktion $\alpha'$ erzeugt (Code F.43)
	\end{itemize}
\end{enumerate}

\paragraph{Entscheidungsfunktion}
\begin{itemize}
	\item Die Entscheidungsfunktion ist das Maximum der utility in der Menge von linearen Funktionen \"uber dem jeweiligen Bereich des Zustandsvermutungsraums
	\item Diese Menge ist immer konvex \"uber dem Zustandsvermutungs-Simplex
\end{itemize}

\paragraph{Komplexit\"at}
\begin{itemize}
	\item Schritt Teil 2 ist aus komplexit\"atstheoretischer Sicht bedeutend:\\
	$\texttt{for each } mset\left[*\right] = (1_1, 1_2, \ldots, 1_M) \textbf{ to } (|\Gamma|_1, \ldots, |\Gamma|_M) \textbf{ do}$
	\item Hier geschieht eine kombinatorische Explosion bez\"uglich der Anzahl der erzeugten $\alpha$:\\
	$|\Gamma_t| = |Actions| * |\Gamma_{t-1}|^{|Observations|}$
	\item Was zu einer doppelt-exponentiellen Komplexit\"at (2-EXPSPACE) f\"uhrt:\\
	$|\Gamma| = |Actions|^{(\sum_{i=0}^{t-1} |Observations|^i)}$
	\item $\Rightarrow$ exakte POMDP Value Iteration unbrauchbar!!
	\item $\Rightarrow$ um die Komplexit\"at handhabbar zu machen, wurden approximative Value Iteration Verfahren entwickelt. Dadurch wird meistens der Teil 2 des Iterationsschritts entsch\"arft. Alle relevanten Verfahren haben garantierte Fehlerschranken und zeichnen sich durch sehr gute Approximation aus
\end{itemize}

\paragraph{Point Based Value Iteration} als approximative Value Iteration
\begin{itemize}
	\item nutzt ein dynamisches Raster zur Berechnung der Entscheidungsfunktion
	\item die Entscheidungsfunktion wird nur f\"ur eine dynamische Menge von Stellvertreterpunkten im Zustandsvermutungsraum berechnet
	\item sie gilt jedoch f\"ur den gesamten Raum
	\item Schema: die Genauigkeit h\"angt in der Praxis von der Anzahl der Vertreterpunkte und der \"Anderungsh\"aufigkeit optimaler Handlungen \"uber dem Zustandsraum ab
\end{itemize}

weitere approximative Value Iterations sind PERSEUS, HSV12, SARSOP..

\subsubsection{POMDPs und Serviceroboter}
\begin{itemize}
	\item die Nutzung von POMDPs in autonomen Servicerobotern steht noch ganz am Anfang
	\item momentan fast nur in Simulationen und virtuellen Benchmarkszenarien im Einsatz
	\item Mono-Modale SLAM-Szenarien stehen dort noch im Mittelpunkt
	\item die \"Ubertragung der Theorie auf Robotersteuerungen wird gerade erforscht (steht zwischen Theoriebildung und Grundlagenforschung, d.h. Translation, angewandte Forschung, Prototypen-Erprobung und Produktionsstart fehlen noch)
	\item Herausforderungen
	\begin{itemize}
		\item \"Uberwindung der Abstraktionsl\"ucke zwischen Roboter Sensorik/Aktorik und symbolischen POMDPs
		\item stochastische Modellierung von Szenarien
		\item Strukturierung der stochastischen Modelle
		\item Integration der St\"arken klassischer Planung/Ausf\"uhrung
		\item Integration von maschinellem Lernen
	\end{itemize}
	\item POMDPs in einer Robotersteuerung (vgl. \autoref{fig:ch08_pomdpRobotersteuerung})
	\begin{itemize}
		\item POMDPs werden mit anderen Steuerungsmethoden zusammen eingesetzt
		\item Schichtung trennt die Methoden
	\end{itemize}
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{figures/ch08_POMPDRobotersteuerung.png}
\caption{POMDPs in einer Robotersteuerung}
\label{fig:ch08_pomdpRobotersteuerung}
\end{figure}